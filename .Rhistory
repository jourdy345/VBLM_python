lambda <- sum(dS * dG) / sum(dG * dG)
S_old <- S_new
S_new <- S_new - lambda * G_new
lower_new <- lower_bound(w, S_new, y, X, mu, Sigma)
if (abs(lower_new - lower_old) < .Machine$double.eps) break
else lower_old <- lower_new
}
return(list('m' = w, 'S' = S_new, 'step' = step, 'lower_bound' = lower_new, 'initial_lower_bound' = initial_lower_bound))
}
setwd('~/Desktop/Computer_related/Variational/linearmodel/VBLM_python/')
library(mvtnorm)
crab <- read.table('crab.txt')
crab <- crab[ ,-1]
crab <- unname(as.matrix(crab))
y <- as.vector(crab[ ,5])
X <- cbind(rep(1, nrow(crab)), crab[ , 1:4])
mu <- as.vector(rmvnorm(1, mean = rep(0, ncol(crab)), sigma = diag(rep(1000, ncol(crab)))))
Sigma <- diag(rep(1000, ncol(crab)))
variational_inference(y, X, mu, Sigma, 40)
e <- X %*% w + 0.5 * diag(X %*% S_new %*% t(X))
w <- rep(1, p)
n <- dim(X)[1]
p <- dim(X)[2]
w <- rep(1, p)
S_old <- matrix(0, nrow = p)
S_new <- diag(rep(1, p))
e <- X %*% w + 0.5 * diag(X %*% S_new %*% t(X))
typeof(e)
is.vector(e)
e
e <- as.vector(X %*% w + 0.5 * diag(X %*% S_new %*% t(X)))
is.vector(e)
hessian_w <- t(X) %*% diag(e) %*% X
w <- w - solve(hessian_w, w_differential(w, S_new, y, X, Sigma))
lower_bound <- function(w, S, y, X, mu, Sigma) {
-(t(y)%*%X%*%w - t(rep(1, n)) %*% exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))) - 0.5 * t((w - mu))%*%solve(Sigma)%*%(w - mu) - 0.5 * sum(diag(solve(Sigma)%*%S)) + 0.5 * log(det(S)) - 0.5 * log(det(Sigma)))
}
w_differential <- function(w, S, y, X, Sigma) {
as.vector(t(y)%*%X - 0.5 * (2 * t(w)%*%solve(Sigma) - 2 * t(mu)%*%solve(Sigma)) - t(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))%*%X)
}
S_differential <- function(w, S, X, Sigma) {
-0.5 * t(X) %*% diag(as.vector(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))) %*% X + 0.5 * t(solve(S)) - 0.5 * solve(Sigma)
}
w <- w - solve(hessian_w, w_differential(w, S_new, y, X, Sigma))
# Optimization of the S matrix is going to be impractical since the Hessian matrix will be (p * p * p * p) 4th-order tensor.
# We instead apply Barzilai-Borwein algorithm (a special type of gradient-based optimization method).
variational_inference <- function(y, X, mu, Sigma, max_iter) {
# mu and Sigma are the mean vector and covariance matrix of the prior distribution of beta
n <- dim(X)[1]
p <- dim(X)[2]
# y <- as.vector(y)
# X <- unname(as.matrix(X))
# mu <- as.vector(mu)
# Sigma <- as.matrix(Sigma)
lower_bound <- function(w, S, y, X, mu, Sigma) {
-(t(y)%*%X%*%w - t(rep(1, n)) %*% exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))) - 0.5 * t((w - mu))%*%solve(Sigma)%*%(w - mu) - 0.5 * sum(diag(solve(Sigma)%*%S)) + 0.5 * log(det(S)) - 0.5 * log(det(Sigma)))
}
w_differential <- function(w, S, y, X, Sigma) {
as.vector(t(y)%*%X - 0.5 * (2 * t(w)%*%solve(Sigma) - 2 * t(mu)%*%solve(Sigma)) - t(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))%*%X)
}
S_differential <- function(w, S, X, Sigma) {
-0.5 * t(X) %*% diag(as.vector(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))) %*% X + 0.5 * t(solve(S)) - 0.5 * solve(Sigma)
}
step <- 0
# initialise w for Newton-Raphson method
w <- rep(1, p)
# initialise 2 S matrices for Barzilai-Borwein method
S_old <- matrix(0, nrow = p)
S_new <- diag(rep(1, p))
# initial lower bound
lower_old <- lower_bound(w, S_new, y, X, mu, Sigma)
initial_lower_bound <- lower_old
for (i in 1:max_iter) {
step <- step + 1
# optimising w (mean vector) ... Newton-Raphson method
e <- as.vector(X %*% w + 0.5 * diag(X %*% S_new %*% t(X)))
hessian_w <- t(X) %*% diag(e) %*% X
w <- w - solve(hessian_w, w_differential(w, S_new, y, X, Sigma))
# optimising S (covariance matrix)
dS <- S_new - S_old
G_old <- S_differential(w, S_old, X, Sigma)
G_new <- S_differential(w, S_new, X, Sigma)
dG <- G_new - G_old
lambda <- sum(dS * dG) / sum(dG * dG)
S_old <- S_new
S_new <- S_new - lambda * G_new
lower_new <- lower_bound(w, S_new, y, X, mu, Sigma)
if (abs(lower_new - lower_old) < .Machine$double.eps) break
else lower_old <- lower_new
}
return(list('m' = w, 'S' = S_new, 'step' = step, 'lower_bound' = lower_new, 'initial_lower_bound' = initial_lower_bound))
}
setwd('~/Desktop/Computer_related/Variational/linearmodel/VBLM_python/')
library(mvtnorm)
crab <- read.table('crab.txt')
crab <- crab[ ,-1]
crab <- unname(as.matrix(crab))
y <- as.vector(crab[ ,5])
X <- cbind(rep(1, nrow(crab)), crab[ , 1:4])
mu <- as.vector(rmvnorm(1, mean = rep(0, ncol(crab)), sigma = diag(rep(1000, ncol(crab)))))
Sigma <- diag(rep(1000, ncol(crab)))
variational_inference(y, X, mu, Sigma, 40)
S_old <- matrix(0, nrow = p)
S_new <- diag(rep(1, p))
dim(S_old)
dim(S_new)
S_old <- matrix(0, nrow = p, ncol = p)
S_new <- diag(rep(1, p))
dim(S_old)
dim(S_new)
# Optimization of the S matrix is going to be impractical since the Hessian matrix will be (p * p * p * p) 4th-order tensor.
# We instead apply Barzilai-Borwein algorithm (a special type of gradient-based optimization method).
variational_inference <- function(y, X, mu, Sigma, max_iter) {
# mu and Sigma are the mean vector and covariance matrix of the prior distribution of beta
n <- dim(X)[1]
p <- dim(X)[2]
# y <- as.vector(y)
# X <- unname(as.matrix(X))
# mu <- as.vector(mu)
# Sigma <- as.matrix(Sigma)
lower_bound <- function(w, S, y, X, mu, Sigma) {
-(t(y)%*%X%*%w - t(rep(1, n)) %*% exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))) - 0.5 * t((w - mu))%*%solve(Sigma)%*%(w - mu) - 0.5 * sum(diag(solve(Sigma)%*%S)) + 0.5 * log(det(S)) - 0.5 * log(det(Sigma)))
}
w_differential <- function(w, S, y, X, Sigma) {
as.vector(t(y)%*%X - 0.5 * (2 * t(w)%*%solve(Sigma) - 2 * t(mu)%*%solve(Sigma)) - t(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))%*%X)
}
S_differential <- function(w, S, X, Sigma) {
-0.5 * t(X) %*% diag(as.vector(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))) %*% X + 0.5 * t(solve(S)) - 0.5 * solve(Sigma)
}
step <- 0
# initialise w for Newton-Raphson method
w <- rep(1, p)
# initialise 2 S matrices for Barzilai-Borwein method
S_old <- matrix(0, nrow = p, ncol = p)
S_new <- diag(rep(1, p))
# initial lower bound
lower_old <- lower_bound(w, S_new, y, X, mu, Sigma)
initial_lower_bound <- lower_old
for (i in 1:max_iter) {
step <- step + 1
# optimising w (mean vector) ... Newton-Raphson method
e <- as.vector(X %*% w + 0.5 * diag(X %*% S_new %*% t(X)))
hessian_w <- t(X) %*% diag(e) %*% X
w <- w - solve(hessian_w, w_differential(w, S_new, y, X, Sigma))
# optimising S (covariance matrix)
dS <- S_new - S_old
G_old <- S_differential(w, S_old, X, Sigma)
G_new <- S_differential(w, S_new, X, Sigma)
dG <- G_new - G_old
lambda <- sum(dS * dG) / sum(dG * dG)
S_old <- S_new
S_new <- S_new - lambda * G_new
lower_new <- lower_bound(w, S_new, y, X, mu, Sigma)
if (abs(lower_new - lower_old) < .Machine$double.eps) break
else lower_old <- lower_new
}
return(list('m' = w, 'S' = S_new, 'step' = step, 'lower_bound' = lower_new, 'initial_lower_bound' = initial_lower_bound))
}
setwd('~/Desktop/Computer_related/Variational/linearmodel/VBLM_python/')
library(mvtnorm)
crab <- read.table('crab.txt')
crab <- crab[ ,-1]
crab <- unname(as.matrix(crab))
y <- as.vector(crab[ ,5])
X <- cbind(rep(1, nrow(crab)), crab[ , 1:4])
mu <- as.vector(rmvnorm(1, mean = rep(0, ncol(crab)), sigma = diag(rep(1000, ncol(crab)))))
Sigma <- diag(rep(1000, ncol(crab)))
variational_inference(y, X, mu, Sigma, 40)
# Optimization of the S matrix is going to be impractical since the Hessian matrix will be (p * p * p * p) 4th-order tensor.
# We instead apply Barzilai-Borwein algorithm (a special type of gradient-based optimization method).
variational_inference <- function(y, X, mu, Sigma, max_iter) {
# mu and Sigma are the mean vector and covariance matrix of the prior distribution of beta
n <- dim(X)[1]
p <- dim(X)[2]
# y <- as.vector(y)
# X <- unname(as.matrix(X))
# mu <- as.vector(mu)
# Sigma <- as.matrix(Sigma)
lower_bound <- function(w, S, y, X, mu, Sigma) {
-(t(y)%*%X%*%w - t(rep(1, n)) %*% exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))) - 0.5 * t((w - mu))%*%solve(Sigma)%*%(w - mu) - 0.5 * sum(diag(solve(Sigma)%*%S)) + 0.5 * log(det(S)) - 0.5 * log(det(Sigma)))
}
w_differential <- function(w, S, y, X, Sigma) {
as.vector(t(y)%*%X - 0.5 * (2 * t(w)%*%solve(Sigma) - 2 * t(mu)%*%solve(Sigma)) - t(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))%*%X)
}
S_differential <- function(w, S, X, Sigma) {
-0.5 * t(X) %*% diag(as.vector(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))) %*% X + 0.5 * t(solve(S)) - 0.5 * solve(Sigma)
}
step <- 0
# initialise w for Newton-Raphson method
w <- rep(1, p)
# initialise 2 S matrices for Barzilai-Borwein method
S_old <- matrix(0, nrow = p, ncol = p)
S_new <- diag(rep(1, p))
# initial lower bound
lower_old <- lower_bound(w, S_new, y, X, mu, Sigma)
initial_lower_bound <- lower_old
for (i in 1:max_iter) {
step <- step + 1
# optimising w (mean vector) ... Newton-Raphson method
e <- as.vector(X %*% w + 0.5 * diag(X %*% S_new %*% t(X)))
hessian_w <- t(X) %*% diag(e) %*% X
w <- w - solve(hessian_w, w_differential(w, S_new, y, X, Sigma))
# optimising S (covariance matrix)
dS <- S_new - S_old
G_old <- S_differential(w, S_old, X, Sigma)
G_new <- S_differential(w, S_new, X, Sigma)
dG <- G_new - G_old
lambda <- sum(dS * dG) / sum(dG * dG)
S_old <- S_new
S_new <- S_new - lambda * G_new
print(S_new)
lower_new <- lower_bound(w, S_new, y, X, mu, Sigma)
if (abs(lower_new - lower_old) < .Machine$double.eps) break
else lower_old <- lower_new
}
return(list('m' = w, 'S' = S_new, 'step' = step, 'lower_bound' = lower_new, 'initial_lower_bound' = initial_lower_bound))
}
setwd('~/Desktop/Computer_related/Variational/linearmodel/VBLM_python/')
library(mvtnorm)
crab <- read.table('crab.txt')
crab <- crab[ ,-1]
crab <- unname(as.matrix(crab))
y <- as.vector(crab[ ,5])
X <- cbind(rep(1, nrow(crab)), crab[ , 1:4])
mu <- as.vector(rmvnorm(1, mean = rep(0, ncol(crab)), sigma = diag(rep(1000, ncol(crab)))))
Sigma <- diag(rep(1000, ncol(crab)))
variational_inference(y, X, mu, Sigma, 40)
S_new
solve(S_new)
dS <- S_new - S_old
G_old <- S_differential(w, S_old, X, Sigma)
# Optimization of the S matrix is going to be impractical since the Hessian matrix will be (p * p * p * p) 4th-order tensor.
# We instead apply Barzilai-Borwein algorithm (a special type of gradient-based optimization method).
variational_inference <- function(y, X, mu, Sigma, max_iter) {
# mu and Sigma are the mean vector and covariance matrix of the prior distribution of beta
n <- dim(X)[1]
p <- dim(X)[2]
# y <- as.vector(y)
# X <- unname(as.matrix(X))
# mu <- as.vector(mu)
# Sigma <- as.matrix(Sigma)
lower_bound <- function(w, S, y, X, mu, Sigma) {
-(t(y)%*%X%*%w - t(rep(1, n)) %*% exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))) - 0.5 * t((w - mu))%*%solve(Sigma)%*%(w - mu) - 0.5 * sum(diag(solve(Sigma)%*%S)) + 0.5 * log(det(S)) - 0.5 * log(det(Sigma)))
}
w_differential <- function(w, S, y, X, Sigma) {
as.vector(t(y)%*%X - 0.5 * (2 * t(w)%*%solve(Sigma) - 2 * t(mu)%*%solve(Sigma)) - t(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))%*%X)
}
S_differential <- function(w, S, X, Sigma) {
-0.5 * t(X) %*% diag(as.vector(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))) %*% X + 0.5 * t(solve(S)) - 0.5 * solve(Sigma)
}
step <- 0
# initialise w for Newton-Raphson method
w <- rep(1, p)
# initialise 2 S matrices for Barzilai-Borwein method
S_old <- diag(rep(1, p))
S_new <- diag(rep(1, p))
# initial lower bound
lower_old <- lower_bound(w, S_new, y, X, mu, Sigma)
initial_lower_bound <- lower_old
for (i in 1:max_iter) {
step <- step + 1
# optimising w (mean vector) ... Newton-Raphson method
e <- as.vector(X %*% w + 0.5 * diag(X %*% S_new %*% t(X)))
hessian_w <- t(X) %*% diag(e) %*% X
w <- w - solve(hessian_w, w_differential(w, S_new, y, X, Sigma))
# optimising S (covariance matrix)
dS <- S_new - S_old
G_old <- S_differential(w, S_old, X, Sigma)
G_new <- S_differential(w, S_new, X, Sigma)
dG <- G_new - G_old
lambda <- sum(dS * dG) / sum(dG * dG)
S_old <- S_new
S_new <- S_new - lambda * G_new
print(S_new)
lower_new <- lower_bound(w, S_new, y, X, mu, Sigma)
if (abs(lower_new - lower_old) < .Machine$double.eps) break
else lower_old <- lower_new
}
return(list('m' = w, 'S' = S_new, 'step' = step, 'lower_bound' = lower_new, 'initial_lower_bound' = initial_lower_bound))
}
setwd('~/Desktop/Computer_related/Variational/linearmodel/VBLM_python/')
library(mvtnorm)
crab <- read.table('crab.txt')
crab <- crab[ ,-1]
crab <- unname(as.matrix(crab))
y <- as.vector(crab[ ,5])
X <- cbind(rep(1, nrow(crab)), crab[ , 1:4])
mu <- as.vector(rmvnorm(1, mean = rep(0, ncol(crab)), sigma = diag(rep(1000, ncol(crab)))))
Sigma <- diag(rep(1000, ncol(crab)))
variational_inference(y, X, mu, Sigma, 40)
G_new <- S_differential(w, S_new, X, Sigma)
G_old <- S_differential(w, S_old, X, Sigma)
S_old <- diag(rep(1, p))
G_old <- S_differential(w, S_old, X, Sigma)
G_OLD
G_old
S_old <- diag(rep(1000, p))
S_new <- diag(rep(1000, p))
G_old <- S_differential(w, S_old, X, Sigma)
G_new <- S_differential(w, S_new, X, Sigma)
G_old
install.packages('dlm')
library(dlm)
rwishart(df = 100, p = p)
S_old <- rwishart(df = 100, p = p)
S_new <- rwishart(df = 100, p = p)
G_old <- S_differential(w, S_old, X, Sigma)
G_new <- S_differential(w, S_new, X, Sigma)
G_old
w
hessian_w
is.vector(W)
is.vector(w)
w
X
# Optimization of the S matrix is going to be impractical since the Hessian matrix will be (p * p * p * p) 4th-order tensor.
# We instead apply Barzilai-Borwein algorithm (a special type of gradient-based optimization method).
variational_inference <- function(y, X, mu, Sigma, max_iter) {
# mu and Sigma are the mean vector and covariance matrix of the prior distribution of beta
n <- dim(X)[1]
p <- dim(X)[2]
# y <- as.vector(y)
# X <- unname(as.matrix(X))
# mu <- as.vector(mu)
# Sigma <- as.matrix(Sigma)
lower_bound <- function(w, S, y, X, mu, Sigma) {
-(t(y)%*%X%*%w - t(rep(1, n)) %*% exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))) - 0.5 * t((w - mu))%*%solve(Sigma)%*%(w - mu) - 0.5 * sum(diag(solve(Sigma)%*%S)) + 0.5 * log(det(S)) - 0.5 * log(det(Sigma)))
}
w_differential <- function(w, S, y, X, Sigma) {
as.vector(t(y)%*%X - 0.5 * (2 * t(w)%*%solve(Sigma) - 2 * t(mu)%*%solve(Sigma)) - t(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))%*%X)
}
S_differential <- function(w, S, X, Sigma) {
-0.5 * t(X) %*% diag(as.vector(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))) %*% X + 0.5 * t(solve(S)) - 0.5 * solve(Sigma)
}
step <- 0
# initialise w for Newton-Raphson method
w <- rep(1, p)
# initialise 2 S matrices for Barzilai-Borwein method
S_old <- rwishart(df = 100, p = p)
S_new <- rwishart(df = 100, p = p)
# initial lower bound
lower_old <- lower_bound(w, S_new, y, X, mu, Sigma)
initial_lower_bound <- lower_old
for (i in 1:max_iter) {
step <- step + 1
# optimising w (mean vector) ... Newton-Raphson method
e <- as.vector(X %*% w + 0.5 * diag(X %*% S_new %*% t(X)))
hessian_w <- t(X) %*% diag(e) %*% X - solve(Sigma)
w <- w - solve(hessian_w, w_differential(w, S_new, y, X, Sigma))
# optimising S (covariance matrix)
dS <- S_new - S_old
G_old <- S_differential(w, S_old, X, Sigma)
G_new <- S_differential(w, S_new, X, Sigma)
dG <- G_new - G_old
lambda <- sum(dS * dG) / sum(dG * dG)
S_old <- S_new
S_new <- S_new - lambda * G_new
print(S_new)
lower_new <- lower_bound(w, S_new, y, X, mu, Sigma)
if (abs(lower_new - lower_old) < .Machine$double.eps) break
else lower_old <- lower_new
}
return(list('m' = w, 'S' = S_new, 'step' = step, 'lower_bound' = lower_new, 'initial_lower_bound' = initial_lower_bound))
}
setwd('~/Desktop/Computer_related/Variational/linearmodel/VBLM_python/')
library(mvtnorm)
crab <- read.table('crab.txt')
crab <- crab[ ,-1]
crab <- unname(as.matrix(crab))
y <- as.vector(crab[ ,5])
X <- cbind(rep(1, nrow(crab)), crab[ , 1:4])
mu <- as.vector(rmvnorm(1, mean = rep(0, ncol(crab)), sigma = diag(rep(1000, ncol(crab)))))
Sigma <- diag(rep(1000, ncol(crab)))
variational_inference(y, X, mu, Sigma, 40)
ㅌ
X
y
# Optimization of the S matrix is going to be impractical since the Hessian matrix will be (p * p * p * p) 4th-order tensor.
# We instead apply Barzilai-Borwein algorithm (a special type of gradient-based optimization method).
variational_inference <- function(y, X, mu, Sigma, max_iter) {
# mu and Sigma are the mean vector and covariance matrix of the prior distribution of beta
n <- dim(X)[1]
p <- dim(X)[2]
# y <- as.vector(y)
# X <- unname(as.matrix(X))
# mu <- as.vector(mu)
# Sigma <- as.matrix(Sigma)
lower_bound <- function(w, S, y, X, mu, Sigma) {
-sum(y * X%*%w) - sum(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X)))) - 0.5 * sum((w - mu) * solve(Sigma)%*%(w - mu)) - 0.5 * sum(diag(solve(Sigma)%*%S)) + 0.5 * log(det(S)) - 0.5 * log(det(Sigma))) + 0.5 * p - sum(log(factorial(y)))
}
w_differential <- function(w, S, y, X, Sigma) {
as.vector(t(y)%*%X - 0.5 * (2 * t(w)%*%solve(Sigma) - 2 * t(mu)%*%solve(Sigma)) - t(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))%*%X)
}
S_differential <- function(w, S, X, Sigma) {
-0.5 * t(X) %*% diag(as.vector(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))) %*% X + 0.5 * t(solve(S)) - 0.5 * solve(Sigma)
}
step <- 0
# initialise w for Newton-Raphson method
w <- rep(1, p)
# initialise 2 S matrices for Barzilai-Borwein method
library(dlm)
S_old <- rwishart(df = 100, p = p)
S_new <- rwishart(df = 100, p = p)
# initial lower bound
lower_old <- lower_bound(w, S_new, y, X, mu, Sigma)
initial_lower_bound <- lower_old
for (i in 1:max_iter) {
step <- step + 1
# optimising w (mean vector) ... Newton-Raphson method
e <- as.vector(X %*% w + 0.5 * diag(X %*% S_new %*% t(X)))
hessian_w <- t(X) %*% diag(e) %*% X - solve(Sigma)
w <- w - solve(hessian_w, w_differential(w, S_new, y, X, Sigma))
# optimising S (covariance matrix)
dS <- S_new - S_old
G_old <- S_differential(w, S_old, X, Sigma)
G_new <- S_differential(w, S_new, X, Sigma)
dG <- G_new - G_old
lambda <- sum(dS * dG) / sum(dG * dG)
S_old <- S_new
S_new <- S_new - lambda * G_new
print(S_new)
lower_new <- lower_bound(w, S_new, y, X, mu, Sigma)
if (abs(lower_new - lower_old) < .Machine$double.eps) break
else lower_old <- lower_new
}
return(list('m' = w, 'S' = S_new, 'step' = step, 'lower_bound' = lower_new, 'initial_lower_bound' = initial_lower_bound))
}
setwd('~/Desktop/Computer_related/Variational/linearmodel/VBLM_python/')
library(mvtnorm)
crab <- read.table('crab.txt')
crab <- crab[ ,-1]
crab <- unname(as.matrix(crab))
y <- as.vector(crab[ ,5])
X <- cbind(rep(1, nrow(crab)), crab[ , 1:4])
mu <- as.vector(rmvnorm(1, mean = rep(0, ncol(crab)), sigma = diag(rep(1000, ncol(crab)))))
Sigma <- diag(rep(1000, ncol(crab)))
variational_inference(y, X, mu, Sigma, 40)
# Optimization of the S matrix is going to be impractical since the Hessian matrix will be (p * p * p * p) 4th-order tensor.
# We instead apply Barzilai-Borwein algorithm (a special type of gradient-based optimization method).
variational_inference <- function(y, X, mu, Sigma, max_iter) {
# mu and Sigma are the mean vector and covariance matrix of the prior distribution of beta
n <- dim(X)[1]
p <- dim(X)[2]
# y <- as.vector(y)
# X <- unname(as.matrix(X))
# mu <- as.vector(mu)
# Sigma <- as.matrix(Sigma)
lower_bound <- function(w, S, y, X, mu, Sigma) {
-sum(y * X%*%w) - sum(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X)))) - 0.5 * sum((w - mu) * solve(Sigma)%*%(w - mu)) - 0.5 * sum(diag(solve(Sigma)%*%S)) + 0.5 * log(det(S)) - 0.5 * log(det(Sigma))) + 0.5 * p - sum(log(factorial(y)))
}
w_differential <- function(w, S, y, X, Sigma) {
as.vector(t(y)%*%X - 0.5 * (2 * t(w)%*%solve(Sigma) - 2 * t(mu)%*%solve(Sigma)) - t(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))%*%X)
}
S_differential <- function(w, S, X, Sigma) {
-0.5 * t(X) %*% diag(as.vector(exp(X%*%w + 0.5 * diag(X%*%S%*%t(X))))) %*% X + 0.5 * t(solve(S)) - 0.5 * solve(Sigma)
}
step <- 0
# initialise w for Newton-Raphson method
w <- rep(1, p)
# initialise 2 S matrices for Barzilai-Borwein method
library(dlm)
S_old <- rwishart(df = 100, p = p)
S_new <- rwishart(df = 100, p = p)
# initial lower bound
lower_old <- lower_bound(w, S_new, y, X, mu, Sigma)
initial_lower_bound <- lower_old
for (i in 1:max_iter) {
step <- step + 1
# optimising w (mean vector) ... Newton-Raphson method
e <- as.vector(X %*% w + 0.5 * diag(X %*% S_new %*% t(X)))
hessian_w <- t(X) %*% diag(e) %*% X - solve(Sigma)
w <- w - solve(hessian_w, w_differential(w, S_new, y, X, Sigma))
# optimising S (covariance matrix)
dS <- S_new - S_old
G_old <- S_differential(w, S_old, X, Sigma)
G_new <- S_differential(w, S_new, X, Sigma)
dG <- G_new - G_old
lambda <- sum(dS * dG) / sum(dG * dG)
S_old <- S_new
S_new <- S_new - lambda * G_new
print(S_new)
lower_new <- lower_bound(w, S_new, y, X, mu, Sigma)
if (abs(lower_new - lower_old) < .Machine$double.eps) break
else lower_old <- lower_new
}
return(list('m' = w, 'S' = S_new, 'step' = step, 'lower_bound' = lower_new, 'initial_lower_bound' = initial_lower_bound))
}
setwd('~/Desktop/Computer_related/Variational/linearmodel/VBLM_python/')
library(mvtnorm)
crab <- read.table('crab.txt')
crab <- crab[ ,-1]
crab <- unname(as.matrix(crab))
y <- as.vector(crab[ ,5])
X <- cbind(rep(1, nrow(crab)), crab[ , 1:4])
mu <- as.vector(rmvnorm(1, mean = rep(0, ncol(crab)), sigma = diag(rep(1000, ncol(crab)))))
Sigma <- diag(rep(1000, ncol(crab)))
variational_inference(y, X, mu, Sigma, 40)
y
factorial(y)
log(factorial(y))
sum(log(factorial(y)))
X
t(X)%*%X
S
S_new
S_new <- matrix(runif(p*p), nrow=p)
S_old <- matrix(runif(p*p), nrow=p)
temp <- exp(X%*%w + 0.5 * diag(X%*%S_new%*%t(X)))
temp
t(X)%*%diag(temp)%*%X
diag(temp)
t(X)%*%diag(as.vector(temp))%*%X
